{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from gymnasium import spaces\n",
    "from tqdm import trange\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from notebooks.datasets.SP100Stocks import SP100Stocks\n",
    "from notebooks.models import A3TGCN\n",
    "from notebooks.ppo import PPO"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:50:37.870247500Z",
     "start_time": "2024-05-13T13:50:12.868531800Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S&P100 weights optimization via Deep Reinforcement Learning\n",
    "The goal of this notebook is to build an agent capable of learning the optimal weights for the S&P100 index that give the best returns. The agent will try to outperform an equally weighted portfolio, meaning that each stock has the same weight."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66fe37191902c89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "The data from the custom PyG dataset for containing the historical prices and the graph structure of the stocks is loaded.\n",
    "For this task, a longer time series than the previous ones is needed. Here 200 days of data is considered, which corresponds to 40 weeks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db076977833be248"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(SP100Stocks(1208),\n Data(x=[100, 5, 25], edge_index=[2, 524], y=[100, 1], edge_weight=[524], close_price=[100, 25], close_price_y=[100, 1]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_duration = 200\n",
    "seq_len = 25\n",
    "dataset = SP100Stocks(past_window=seq_len)\n",
    "dataset, dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:50:39.234390300Z",
     "start_time": "2024-05-13T13:50:37.869285800Z"
    }
   },
   "id": "42734ad9d172b18",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is split into a training and testing set. The testing contains the last `seq_len` days of the dataset. The training set contains the rest of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e9b0b8a7014147"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training episodes: 100%|██████████| 101/101 [00:25<00:00,  3.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(101, SP100Stocks(200), SP100Stocks(200))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_episodes has the shape (n_episodes, episode_duration, n_stocks, n_features, seq_len)\n",
    "train_episodes = [\n",
    "\tdataset[i:i+episode_duration] for i in trange(0, len(dataset)-episode_duration, 10, desc='Creating training episodes', position=0)\n",
    "]\n",
    "random.shuffle(train_episodes)\n",
    "\n",
    "test_episode = dataset[-episode_duration:]\n",
    "len(train_episodes), train_episodes[0], test_episode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:05.489315600Z",
     "start_time": "2024-05-13T13:50:39.234390300Z"
    }
   },
   "id": "f85f8300c768b01",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reinforcement Learning Environment\n",
    "The environment is defined using the Gymnasium API.\n",
    "- State\n",
    "\n",
    "The observations are the graphs of the stocks at each time step, where nodes contain data on the last `n` days (variation, price, RSI, ...).\n",
    "- Action\n",
    "\n",
    "The action space is a continuous space of size (n_stocks,) where the agent can choose the weights for each stock. We need $\\sum_{i=1}^{n} a_i = 1$, where $a_i$ is the weight of stock $i$. A simple way to achieve this is to apply a softmax on the received action.\n",
    "- Reward\n",
    "\n",
    "The goal of the agent is to beat the baseline, which is an equally weighted portfolio. The reward is a function of the difference between the agent's performance and the baseline's performance. The reward is calculated with a sigmoid function as follows:\n",
    "$$\n",
    "r = \\left( \\frac{1}{1 + e^{b - a}} - 0.5 \\right) \\times c\n",
    "$$\n",
    "where $b$ is the baseline performance, $a$ is the agent's performance, and $c$ is a reward coefficient."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "923bbdb1dccfa115"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SP100Env(gym.Env):\n",
    "\tmetadata = {'render.modes': ['human']}\n",
    "\tdef __init__(self, n_stocks: int, n_edges: int, n_features, seq_len: int, train_episodes: list[SP100Stocks], test_episode: SP100Stocks, render_mode: str = None, reward_coef: float = 50):\n",
    "\t\tsuper(SP100Env, self).__init__()\n",
    "\t\tself.n_stocks: int = n_stocks\n",
    "\t\tself.n_features: int = n_features\n",
    "\t\tself.seq_len: int = seq_len\n",
    "\t\tself.episode_length: int = len(train_episodes[0]) - 1  # -1 because the next day is needed to calculate the reward\n",
    "\t\tself.reward_coef: float = reward_coef\n",
    "\t\t\n",
    "\t\tself.train_episodes: list[SP100Stocks] = train_episodes\n",
    "\t\tself.test_episode: list[SP100Stocks] = test_episode\n",
    "\t\t\n",
    "\t\tself.current_episode: SP100Stocks = train_episodes[0]\n",
    "\t\tself.current_step: int = 0\n",
    "\t\t\n",
    "\t\tassert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "\t\tself.render_mode: str = render_mode\n",
    "\n",
    "\t\tself.action_space: spaces.Box = spaces.Box(low=0, high=1, shape=(n_stocks,), dtype=np.float32)\n",
    "\t\tself.observation_space: spaces.Dict = spaces.Dict({\n",
    "\t\t\t'x': spaces.Box(low=-np.inf, high=np.inf, shape=(n_stocks, n_features, seq_len), dtype=np.float32),\n",
    "\t\t\t'edge_index': spaces.Box(low=0, high=n_stocks-1, shape=(2, n_edges), dtype=np.int64),\n",
    "\t    'edge_weight': spaces.Box(low=-np.inf, high=np.inf, shape=(n_edges,), dtype=np.float32),\n",
    "\t\t})\n",
    "\t\t\n",
    "\t\tself.baseline_weights: np.ndarray = np.ones(n_stocks) / n_stocks\n",
    "\t\tself.current_baseline_performance: float = 1.\n",
    "\t\tself.current_agent_performance: float = 1.\n",
    "\t\t\n",
    "\tdef reset(self, seed=None, options=None) -> dict:\n",
    "\t\tif options and options.get(\"test\", False):\n",
    "\t\t\tself.current_episode = self.test_episode\n",
    "\t\telse:\n",
    "\t\t\tself.current_episode = random.choice(self.train_episodes)\n",
    "\t\tself.current_step = 0\n",
    "\t\tself.current_baseline_performance = 1.\n",
    "\t\tself.current_agent_performance = 1.\n",
    "\t\t\n",
    "\t\tself.render()\n",
    "\t\treturn self._get_observation(), {}\n",
    "\t\n",
    "\tdef step(self, action: np.ndarray) -> tuple[dict, float, bool, dict]:\n",
    "\t\taction = np.exp(action) / np.sum(np.exp(action))  # softmax to ensure weights sum to 1\n",
    "\t\tassert self.action_space.contains(action)\n",
    "\t\tassert action.shape == (self.n_stocks,)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tnext_day_variations = self.current_episode[self.current_step + 1].x[:, 2, -1].numpy() / 100  # variations are in percentage\n",
    "\t\t\n",
    "\t\tbaseline_variations = np.sum(next_day_variations * self.baseline_weights)\n",
    "\t\tagent_variations = np.sum(next_day_variations * action)\n",
    "\t\t\n",
    "\t\tself.current_baseline_performance *= (1 + baseline_variations)\n",
    "\t\tself.current_agent_performance *= (1 + agent_variations)\n",
    "\t\t\t\t\n",
    "\t\tself.current_step += 1\n",
    "\t\tobservation = self._get_observation()\n",
    "\t\treward = self._get_reward(baseline_variations, agent_variations)\n",
    "\t\tterminated = self.current_step >= self.episode_length\n",
    "\t\t\n",
    "\t\tself.render()\n",
    "\t\t\n",
    "\t\tif terminated:\n",
    "\t\t\tself.reset()\n",
    "\t\t\n",
    "\t\treturn observation, reward, terminated, False, {}\n",
    "\t\n",
    "\tdef render(self) -> None:\n",
    "\t\tif self.render_mode == 'human':\n",
    "\t\t\tprint(f'Current step: {self.current_step}/{self.episode_length}')\n",
    "\t\t\tprint(f'Baseline performance: {self.current_baseline_performance}')\n",
    "\t\t\tprint(f'Agent performance: {self.current_agent_performance}')\n",
    "\n",
    "\tdef close(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef _get_observation(self) -> dict:\n",
    "\t\treturn {\n",
    "\t\t\t'x': self.current_episode[self.current_step].x.numpy(),\n",
    "\t\t\t'edge_index': self.current_episode[self.current_step].edge_index.numpy(),\n",
    "\t\t\t'edge_weight': self.current_episode[self.current_step].edge_weight.numpy(),\n",
    "\t\t}\n",
    "\t\n",
    "\tdef _get_reward(self, step_baseline_performance: float, step_agent_performance: float) -> float:\n",
    "\t\treturn (1 / (1 + np.exp(step_baseline_performance - step_agent_performance)) - .5) * self.reward_coef"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:05.518312500Z",
     "start_time": "2024-05-13T13:51:05.500286400Z"
    }
   },
   "id": "ab24cedc19d048f1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_stocks, n_edges, n_features = dataset[0].x.shape[0], dataset[0].edge_index.shape[1], dataset[0].x.shape[1]\n",
    "\n",
    "env = SP100Env(n_stocks=n_stocks, n_edges=n_edges,n_features=n_features, seq_len=seq_len, train_episodes=train_episodes, test_episode=test_episode)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:06.251644200Z",
     "start_time": "2024-05-13T13:51:05.510334500Z"
    }
   },
   "id": "205f263d794a6482",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the agent\n",
    "The agent uses the A3T-GCN model. The action space is a continuous space of size (n_stocks,). An Actor-Critic model is used. The actor will output a Gaussian distribution with a mean and a standard deviation of size (n_stocks,). The critic will output the value of the state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8fefc37273cc016"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, n_stocks: int, n_features: int, hidden_size: int, layers_nb: int):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\t\tself.mean = A3TGCN(n_features, 1, hidden_size, layers_nb)\n",
    "\t\tself.logstd = nn.Parameter(torch.zeros(n_stocks))\n",
    "\t\n",
    "\tdef forward(self, obs: Data | Batch) -> tuple[torch.tensor, torch.tensor]:\n",
    "\t\treturn self.mean(obs.x, obs.edge_index, obs.edge_weight).view(obs.num_graphs, -1), torch.exp(self.logstd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:06.266275400Z",
     "start_time": "2024-05-13T13:51:06.255428900Z"
    }
   },
   "id": "a1cf2d9e5d53e219",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, n_stocks: int, n_features: int, hidden_size: int, layers_nb: int):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\t\tself.value = A3TGCN(n_features, 1, hidden_size, layers_nb)\n",
    "\t\tself.out = nn.Linear(n_stocks, 1)\n",
    "\t\n",
    "\tdef forward(self, obs: Data | Batch) -> torch.tensor:\n",
    "\t\tvalues = self.value(obs.x, obs.edge_index, obs.edge_weight).view(obs.num_graphs, -1)\n",
    "\t\treturn self.out(values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:06.269267700Z",
     "start_time": "2024-05-13T13:51:06.261923800Z"
    }
   },
   "id": "97ec4f01dfd98f76",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiating the agent & algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08b15dbbbe6f990"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "actor_lr, critic_lr = 1e-3, 3e-3\n",
    "\n",
    "actor = Actor(n_stocks, n_features, hidden_size, 2)\n",
    "critic = Critic(n_stocks, n_features, hidden_size, 2)\n",
    "\n",
    "ppo = PPO(config={\n",
    "\t\"env_fn\": lambda: SP100Env(n_stocks=n_stocks, n_edges=n_edges, n_features=n_features, seq_len=seq_len, train_episodes=train_episodes, test_episode=test_episode),\n",
    "\t\"num_envs\": 2,\n",
    "\t\n",
    "\t\"actor_model\": actor,\n",
    "\t\"critic_model\": critic,\n",
    "\t\n",
    "\t\"horizon\": 16,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T13:51:06.364163800Z",
     "start_time": "2024-05-13T13:51:06.270265300Z"
    }
   },
   "id": "c7b6cae645f5db30",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29c8ca380b033e88"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== STARTING TRAINING ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Training:  11%|█         | 1103/10000 [21:16<1:17:25,  1.92it/s] "
     ]
    }
   ],
   "source": [
    "ppo.train(max_steps=10_000, task_title=\"S&P100WeightsOptimization\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-13T13:51:06.365159100Z"
    }
   },
   "id": "c0e03f2c91c5226",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The trained agent is tested on the testing set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ba72b7f327788b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "agent_performance, baseline_performance = [], []\n",
    "\n",
    "obs, infos = env.reset(options={\"test\": True})\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\tagent_performance.append(env.current_agent_performance)\n",
    "\tbaseline_performance.append(env.current_baseline_performance)\n",
    "\n",
    "\taction = np.ones(n_stocks).astype(np.float32)\n",
    "\tobs, _, done, _, _ = env.step(action)\n",
    "\t\t\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(agent_performance, label='Agent')\n",
    "plt.plot(baseline_performance, label='Baseline')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Portfolio performance')\n",
    "plt.title('Random agent vs Trained agent vs Baseline performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1fc92b3528ce18af",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
